{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softmax activation function\n",
    "\n",
    "The formula for the **softmax activation function** is:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ z_i $ is the $ i $-th input element (logit) in a vector of $ n $ elements.\n",
    "- $ e^{z_i} $ is the exponential of $ z_i $.\n",
    "- $ \\sum_{j=1}^{n} e^{z_j} $ is the sum of the exponentials of all $ n $ input elements.\n",
    "\n",
    "The softmax function normalizes the outputs to produce a probability distribution, where the sum of all outputs is 1, and each output is in the range (0, 1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [4.8 , 1.21 , 2.385]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E = 2.71828182846\n",
    "E = math.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[121.51041751873483, 3.353484652549023, 10.859062664920513]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_values = []\n",
    "for output in layer_outputs:\n",
    "    exp_values.append(E**output)\n",
    "exp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([121.51041752,   3.35348465,  10.85906266])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_values = np.exp(layer_outputs)\n",
    "exp_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "norm_base = sum(exp_values)\n",
    "norm_values = []\n",
    "\n",
    "for value in exp_values:\n",
    "    norm_values.append(value / norm_base)\n",
    "print(norm_values)\n",
    "print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.89528266 0.02470831 0.08000903]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "norm_values = exp_values / np.sum(exp_values)\n",
    "print(norm_values)\n",
    "print(sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = [[4.8 , 1.21 , 2.385],\n",
    "                 [8.9 , -1.81 , 0.2],\n",
    "                 [1.41 , 1.051 , 0.026]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.21510418e+02 3.35348465e+00 1.08590627e+01]\n",
      " [7.33197354e+03 1.63654137e-01 1.22140276e+00]\n",
      " [4.09595540e+00 2.86051020e+00 1.02634095e+00]]\n"
     ]
    }
   ],
   "source": [
    "exp_values = np.exp(layer_outputs)\n",
    "print(exp_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis = None adds up all the values in the matrix , axis = 0 adds up the values of each column , axis = 1 adds up the values of each row\n",
    "keepdims = True , ensures the dimensions so that dot product or division becomes possible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.395]\n",
      " [7.29 ]\n",
      " [2.487]]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(layer_outputs , axis = 1 , keepdims= True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.95282664e-01, 2.47083068e-02, 8.00090293e-02],\n",
       "       [9.99811129e-01, 2.23163963e-05, 1.66554348e-04],\n",
       "       [5.13097164e-01, 3.58333899e-01, 1.28568936e-01]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm_values = exp_values /  np.sum(exp_values , axis= 1 , keepdims= True)\n",
    "norm_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(norm_values , axis = 1 , keepdims= True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we subtract the maximum value in the softmax function?\n",
    "\n",
    "In the softmax function, we often subtract the maximum value of the input $ z $ vector from each element before applying the exponential function. This step is crucial to avoid numerical overflow, especially when dealing with very large values.\n",
    "\n",
    "Consider the softmax formula:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "If any $ z_i $ is large, the exponentiation $ e^{z_i} $ can result in extremely large values, which can cause overflow issues or make the computations unstable. To prevent this, we subtract the maximum value of $ z $ (denoted as $ z_{\\text{max}} $) from all elements:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i - z_{\\text{max}}}}{\\sum_{j=1}^{n} e^{z_j - z_{\\text{max}}}}\n",
    "$$\n",
    "\n",
    "#### Why does this work without changing the result?\n",
    "\n",
    "Subtracting a constant like $ z_{\\text{max}} $ from each element of $ z $ doesn't change the result of the softmax function. This is because the softmax function is scale-invariant to such shifts. Hereâ€™s why:\n",
    "\n",
    "- Both the numerator and denominator are scaled by the same factor, $ e^{-z_{\\text{max}}} $. \n",
    "- This factor cancels out:\n",
    "\n",
    "$$\n",
    "\\sigma(z_i) = \\frac{e^{z_i - z_{\\text{max}}}}{\\sum_{j=1}^{n} e^{z_j - z_{\\text{max}}}} = \\frac{e^{z_i}}{\\sum_{j=1}^{n} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Thus, the result remains the same, but we avoid large exponentials that could lead to overflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
