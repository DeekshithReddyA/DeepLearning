{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_output = [0.7, 0.1 , 0.2]\n",
    "target_output = [1 , 0 , 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = - (math.log(softmax_output[0])*target_output[0] +\n",
    "          math.log(softmax_output[1])*target_output[1] +\n",
    "          math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "loss = -math.log(softmax_output[0])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051565782628\n",
      "0.2876820724517809\n",
      "0.6931471805599453\n",
      "1.6094379124341003\n"
     ]
    }
   ],
   "source": [
    "print(-math.log(0.9))\n",
    "print(-math.log(0.75))\n",
    "print(-math.log(0.5))\n",
    "print(-math.log(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = np.array([[0.7 , 0.1 , 0.2],\n",
    "                            [0.1 , 0.5 , 0.4],\n",
    "                            [0.02 , 0.9 , 0.08]])\n",
    "class_targets = [0 , 1 , 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7  0.1  0.02]\n",
      "[0.7 0.5 0.9]\n",
      "[0.7 0.5 0.9]\n",
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(softmax_outputs[[0 , 1 , 2] , [0,0,0]])\n",
    "print(softmax_outputs[[0 , 1 , 2] , class_targets])\n",
    "print(softmax_outputs[range(len(softmax_outputs)),  class_targets])\n",
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)),  class_targets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Loss Function\n",
    "\n",
    "The **cross-entropy loss** (also known as log loss) is commonly used in classification tasks to measure the difference between the true distribution of labels and the predicted distribution (output from softmax or sigmoid). It quantifies how well the model's predicted probabilities align with the actual labels.\n",
    "\n",
    "For a single prediction, the formula for cross-entropy loss is:\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\sum_{i=1}^{n} y_i \\log(p_i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ y_i $ is the true label (1 if the class is correct, 0 otherwise).\n",
    "- $ p_i $ is the predicted probability for class $i$ (output from the softmax or sigmoid).\n",
    "- $ n $ is the total number of classes.\n",
    "\n",
    "In **binary classification**, the cross-entropy loss simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = -[y \\log(p) + (1 - y) \\log(1 - p)]\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ y $ is the true label (either 0 or 1).\n",
    "- $ p $ is the predicted probability of the positive class.\n",
    "\n",
    "#### Why Cross-Entropy is Effective:\n",
    "- **Low loss for correct predictions**: If the predicted probability for the correct class is close to 1, the log of that value is close to 0, resulting in a small loss.\n",
    "- **High loss for incorrect predictions**: If the model assigns a low probability to the correct class (close to 0), the log of that small value will be very large in magnitude, resulting in a high loss.\n",
    "\n",
    "### The Problem with Confidence Value 0\n",
    "\n",
    "If the predicted probability $ p $ for the correct class is **exactly 0**, the cross-entropy loss becomes problematic. This is because:\n",
    "\n",
    "$$\n",
    "\\log(0) = -\\infty\n",
    "$$\n",
    "\n",
    "Therefore, if a model predicts a probability of **0** for the true class, the loss function will return **infinity**. This can cause major issues:\n",
    "- **Numerical instability**: Loss values approaching infinity can lead to unstable or untrainable models.\n",
    "- **No gradient**: When the loss becomes infinite, the model has difficulty learning because the gradients needed to adjust the model's weights explode or become undefined.\n",
    "\n",
    "### How to Solve the Issue: Using **Smoothing**\n",
    "\n",
    "To avoid the issue of taking the log of 0, we use a technique called **label smoothing** or apply a small constant $ \\epsilon $ to the predicted probabilities. This prevents any probability from being exactly 0 or 1.\n",
    "\n",
    "For example, instead of allowing probabilities to be exactly 0 or 1, we clip them to a very small positive value $ \\epsilon $ such as $ 10^{-15} $:\n",
    "\n",
    "$$\n",
    "p_i = \\max(\\epsilon, \\min(1 - \\epsilon, p_i))\n",
    "$$\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "- The log function never receives a 0 as input.\n",
    "- The loss remains finite and the model can continue learning.\n",
    "\n",
    "### Modified Cross-Entropy Formula with Smoothing:\n",
    "After clipping the probabilities to avoid $ \\log(0) $, the cross-entropy formula becomes:\n",
    "\n",
    "$$\n",
    "\\text{Cross-Entropy Loss} = -\\sum_{i=1}^{n} y_i \\log(\\max(\\epsilon, p_i))\n",
    "$$\n",
    "\n",
    "Where $ \\epsilon $ is a small positive constant (e.g., $ 10^{-15} $) to prevent any log(0) operations.\n",
    "\n",
    "By applying this smoothing or clipping, we can avoid the infinite loss problem and keep the training stable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
